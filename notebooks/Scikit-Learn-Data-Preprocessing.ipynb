{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.631578947368421"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = iris.data[:, :2], iris.target\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, random_state=33)\n",
    "scaler=preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.90162383 0.71305793 0.         0.         0.        ]\n",
      " [0.88823175 0.70300404 0.94943237 0.         0.        ]] \n",
      "\n",
      " [0.88823175 0.70300404 0.94943237 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.random.random((10, 5))\n",
    "y = np.array(['M','M','F','F','M','F','M','M','F','F'])\n",
    "X[X < 0.7] = 0\n",
    "print(X[0:2], '\\n'*2, X[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training And Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 2), (112, 2), (38, 2))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, y = iris.data[:, :2], iris.target\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, random_state=0)\n",
    "X.shape, X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing The Data\n",
    "### Standardization\n",
    "Standardize features by removing the mean and scaling to unit variance.\n",
    "Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.7 3.2] \n",
      "\n",
      " [-1.36797986  0.34131533]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 1.21331516e-15, -4.41115398e-17]), array([1., 1.]))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "std_X_train=scaler.transform(X_train)\n",
    "std_X_test=scaler.transform(X_test)\n",
    "print(X_train[3], '\\n'*2, std_X_train[3])\n",
    "std_X_train.mean(axis=0), std_X_train.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "Normalize samples individually to unit norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.9 3. ]\n",
      " [5.8 2.6]] \n",
      "\n",
      " [[0.89138513 0.45324668]\n",
      " [0.91250932 0.4090559 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "scaler = Normalizer().fit(X_train)\n",
    "norm_X_train = scaler.transform(X_train)\n",
    "norm_X_test = scaler.transform(X_test)\n",
    "print(X_train[:2], '\\n'*2, norm_X_train[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.9 3. ]\n",
      " [5.8 2.6]] \n",
      "\n",
      " [[1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "scaler = Binarizer(threshold=5.0).fit(X_train)\n",
    "bin_X_train = scaler.transform(X_train)\n",
    "bin_X_test = scaler.transform(X_test)\n",
    "print(X_train[:2], '\\n'*2, bin_X_train[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Categorical Features\n",
    "### Label Encoder\n",
    "[Label Encoder vs. One Hot Encoder](https://medium.com/@contactsunny/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 3, 2])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "enc = LabelEncoder()\n",
    "label_y = ['Toronto', 'Ottawa', 'Montreal', 'Vancouver', 'Toronto']\n",
    "lbl_encoded_y = enc.fit_transform(label_y)\n",
    "lbl_encoded_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoder\n",
    "Matrices that contain mostly zero values are called sparse, distinct from matrices where most of the values are non-zero, called dense.\n",
    "1. **Compressed Sparse Row** - represented using 3 one-dimensional arrays for non-zero values, extents of the rows and the column indexes\n",
    "2. **Compressed Sparse Column** - same as CSR except the column indices are compresssed and read before row indices\n",
    "\n",
    "[Introduction to Sparse Matrices](https://machinelearningmastery.com/sparse-matrices-for-machine-learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]] \n",
      "\n",
      " [[0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "hot_encoded_y_dense = OneHotEncoder(sparse=False).fit_transform(np.array(label_y).reshape(len(label_y), 1))\n",
    "hot_encoded_y_dense\n",
    "hot_encoded_y_sparse = OneHotEncoder().fit_transform(np.array(label_y).reshape(len(label_y), 1))\n",
    "print(hot_encoded_y_dense, '\\n'*2, hot_encoded_y_sparse.todense())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing Missing Values\n",
    "Other strategies: most_frequent, constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 3. ],\n",
       "       [2. , 3.5],\n",
       "       [1.5, 4. ]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data={'col1': [1,2,0], 'col2': [3,0,4]})\n",
    "imp = SimpleImputer(missing_values=0, strategy='mean')\n",
    "imp.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Polynomial Features\n",
    "[Why you should generate polynomial features first](https://medium.com/@samchaaa/preprocessing-why-you-should-generate-polynomial-features-first-before-standardizing-892b4326a91d)\n",
    "#### Polynomial features\n",
    "Polynomial features are often created when we want to include the notion that there exists a nonlinear relationship between the features and the target. For example, we might suspect that the effect of age on the probability of having a major medical condition is not constant over time but increases as age increases. We can encode that nonconstant effect in a feature, x, by generating that featureâ€™s higher-order forms (x2, x3, etc.).\n",
    "#### Interaction features\n",
    "Additionally, often we run into situations where the effect of one feature is dependent on another feature. A simple example would be if we were trying to predict whether or not our coffee was sweet and we had two features: 1) whether or not the coffee was stirred and 2) if we added sugar. Individually, each feature does not predict coffee sweetness, but the combination of their effects does. That is, a coffee would only be sweet if the coffee had sugar and was stirred. The effects of each feature on the target (sweetness) are dependent on each other. We can encode that relationship by including an interaction feature that is the product of the individual features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  2.,  4.,  8.],\n",
       "       [ 1.,  3.,  9., 27.],\n",
       "       [ 1.,  4., 16., 64.]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_X = np.array([1,2,3,4])\n",
    "poly = PolynomialFeatures(3)\n",
    "poly.fit_transform(poly_X.reshape(len(poly_X), 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
